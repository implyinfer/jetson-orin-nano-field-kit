version: "3.8"

services:
  rf-inference:
    image: roboflow/roboflow-inference-server-jetson-6.2.0:latest
    container_name: rf-inference
    runtime: nvidia
    read_only: true
    ports:
      - "9001:9001"
    volumes:
      - /var/cache/roboflow:/cache:rw
    tmpfs:
      - /tmp:rw,noexec,nosuid,size=256m
    environment:
      - MODEL_CACHE_DIR=/cache
      - MPLCONFIGDIR=/tmp/matplotlib
      - ONNXRUNTIME_EXECUTION_PROVIDERS=[TensorrtExecutionProvider,CUDAExecutionProvider,CPUExecutionProvider]
      - METRICS_ENABLED=false
      - ENABLE_BUILDER=True
    security_opt:
      - no-new-privileges
    cap_drop:
      - ALL
    cap_add:
      - NET_BIND_SERVICE
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9001/"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 60s
